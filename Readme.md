# Do Metadata and Appearance of the Retrieved Webpages Affect LLM's Reasoning in Retrieval-Augmented Generation?

This repo contains the codes and data used in our paper [Do Metadata and Appearance of the Retrieved Webpages Affect LLM's Reasoning in Retrieval-Augmented Generation?](https://openreview.net/forum?id=8R75mUrWIV&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3Daclweb.org%2FACL%2FARR%2F2024%2FJune%2FAuthors%23your-submissions)).
In this paper, we study how non-textual information in a retrieved webpage affects the LLM's answer when the evidence contains conflicting information.

## Quick Links
- [ConflictingQA-Fake](#conflictingqa-fake)
- [Preparation](#preparation)
- [Text LLM](#text-llm)
- [Vision LLM](#vision-llm)
- [Citation](#citation)

## ConflictingQA-Fake
The ConflictingQA-Fake dataset can be found in `data/conflicting_qa_fake.json`;
we also put pre-processed ConflictingQA from [Wan et al.](https://github.com/AlexWan0/rag-convincingness) in `data/conflicting_qa_with_keyword.json`.

### Codes for Constructing ConflictingQA-Fake
We construct ConflictingQA-Fake using GPT-4.
First, we use the web interface of GPT-4 to generate questions.
Next, we use `utils/generate_fake_paragraph.py` to generate a paragraph and then use `utils/generate_keyword.py` to generate a the title and extract keywords from the paragraph and the keywords.

## Preparation
### Install Packages
We use conda to create a virtual environment.
The packages in this environment is in `requirements.txt`.
To install create conda envioronment and install packages, run
```
conda create -y -n rag-metadata python=3.9
conda activate rag-metadata
pip3 install -r requirements.txt
```

### Setup API Keys
If you want to use OpenAI, Claude, or Gemini models, place your API key in the `api/openai.txt`, `api/claude.txt`, and `google.txt` respectively.

## Text LLM
To run the experiments in Section 3 and Section 4 in our paper, which use text-only LLM, you need to use `text_llm.py`.

### Direct Answer
To use text LLM to generate an answer without CoT, you can run as the follows:

```
python3 text_llm.py \
    --dataset_path data/conflicting_qa_fake.json \
    --url_modifier "$url_modifier" \
    --model_name "$model" \
    --prompt_template "$prompt_template" \
    --favored_stance "$favored_stance" \
    --output_dir results/conflicing_qa_fake_results \
    --modify_meta_data 1
```
The `model_name` should be a model name in Huggingface or a model name of a proprietary model in OpenAI or Claude.
The `prompt_template` is related to what type of non-textual information that should be added.
Valid choices are listed in the following table.

|`prompt_template`|Explanation| Paper Section|
|-|-|-|
|`input_date`| Adding the publication date two documents but does not tell the LLM what date today is.|*no-today* in Section 3.1|
| `input_date_today` | Adding the publication date two documents and also tell the LLM what date today is. We set today to 2024/04/30. If you want to change the date, please manually modify the `utils/prompts.py`. |*today* in Section 3.1|
| `input_emphasize_url` | Adding the source website URL to the documents | *URL* in Section 4.1 |
| `input_emphasize_src` | Adding the source website name to the documents | *name* in Section 4.1 |

The `url_modifier` is only used when adding the website source to the documents.
The available values are related to the `prompt_template` parameter.
If `prompt_template` is `input_emphasize_url`, the available values for `url_modifier` includes `wiki_wordpress_url` and `cnn_naturalnews_url`.
If `prompt_template` is `input_emphasize_src`, the available values for `url_modifier` includes `wiki_wordpress_src` and `cnn_naturalnews_src`.

You can change which dataset to use by changing the `--dataset_path` argument.
Remember to change the `output_dir` when you want to use a different dataset.

The `favored_stance` can be `yes` or `no` and corresponds to the stance of the document is published more recently or from a more reliable source.

### Chain-of-Thought (CoT)
To make the LLM output with CoT, simple add the `--generation` argument.

```
python3 text_llm.py \
    --dataset_path data/conflicting_qa_fake.json \
    --url_modifier "$url_modifier" \
    --model_name "$model" \
    --prompt_template "$prompt_template" \
    --favored_stance "$favored_stance" \
    --output_dir results/conflicing_qa_fake_results \
    --modify_meta_data 1
```

After, this, there should be some files create in the `$output_path/generate` directory.
To use ChatGPT to extract the answer from LLM's length CoT answer, you can use `extract_answer_from_generation.py`.
Example usage:
```
python3 extract_answer_from_generation.py \
    --answer_file results/conflicing_qa_fake_results/generate/"$model"/"$prompt_template"_"$favored_stance".json
```
The `--answer_file` is a file generated by running `text_llm.py`.


## Vision LLM
You can run the experiments of website appearance using `vision_llm.py`.
For example, to run the **direct answer** setting, simply run
```
python3 vision_llm.py \
    --model_name "$model" \
    --max_tokens 512 \
    --prompt_template "$prompt_template" \
    --dataset_path  data/conflicting_qa_fake.json \
    --yes_html_template "$yes_html_template" \
    --no_html_template "$no_html_template" \
    --image_dir data/imgs/fake \
    --output_dir vision_llm_results
```
The model currently only supports proprietary VLLM models including GPT-4 series, Claude-3 series, and Gemini.
The prompt template can be `vision_prompts` (*screenshot* in Section 5.1) or `vision_prompts_with_text` (*screenshot + text* in Section 5.1).
The `yes_html_template` and `no_html_template` can be `pretty` (corresponds to *CSS* template) and `simple` (corresponds to *Raw HTML*).
You can check the screenshots in the `data/imgs/fake` to have a taste of what the VLLM sees as the input.
To generate with CoT, simply add the `--generation` argument.

## Generate the Tables in the Paper
We provide a python script to generate Table 2, 3, and 4 in our paper: `generate_latex_table.py`.
Check the arguments in the script to see what you need to set.
You will need to provide a mode (`date`, `wiki_src`, `cnn_src`) and the result path.
If you need to run the script for vision llm, you need to add the `--vision` argument.
Note that you need to run the `extract_answer_from_generation.py.` before running this script. 
Running the code will also conduct the McNeymar's test.

## Citation
If you use ConflictingQA-Fake or find this repo useful, please cite our paper by
```
@misc{chiang2024do,
      title={Do Metadata and Appearance of the Retrieved Webpages Affect LLM's Reasoning in Retrieval-Augmented Generation?}, 
      author={Cheng-Han Chiang and Hung-yi Lee},
      year={2024},
      eprint={},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

